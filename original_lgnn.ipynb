{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from munch import munchify\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from .data.sbm import SBM_Dataset\n",
    "from .models.losses import combinatorical_accuracy, ari_score\n",
    "\n",
    "sys.path.append('./GNN4CD/src')\n",
    "from load import get_lg_inputs\n",
    "from losses import compute_loss_multiclass\n",
    "from models import lGNN_multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBM_Dataset_adjacency(SBM_Dataset):\n",
    "    def __getitem__(self, idx):\n",
    "        G, labels = super().__getitem__(idx)\n",
    "        A = np.array(nx.adjacency_matrix(G).todense(), dtype=np.float32)\n",
    "        return A, labels\n",
    "\n",
    "n, k, p_in, p_out = 50, 5, 0.8, 0.2\n",
    "n_epoch, n_samples_train, n_samples_test = 3, 500, 100\n",
    "train_dataset = SBM_Dataset_adjacency(n, k, p_in, p_out, n_graphs=n_samples_train)\n",
    "test_dataset = SBM_Dataset_adjacency(n, k, p_in, p_out, n_graphs=n_samples_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "args = munchify({\n",
    "    'clip_grad_norm': 40.0,\n",
    "    'num_features': 8,\n",
    "    'num_layers': 30,\n",
    "    'n_classes': k,\n",
    "    'J': 2,\n",
    "    'lr': 0.004\n",
    "})\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "model = lGNN_multiclass(args.num_features, args.num_layers, args.J + 2, n_classes=args.n_classes).to(device)\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8036594e6de04c4391f304c7d4a20f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='0', max=500, style=ProgressStyle(description_width='initial')â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 50]) torch.Size([1, 50, 50, 4]) torch.Size([1, 50, 1]) torch.Size([1, 734, 734, 4]) torch.Size([1, 734, 1]) torch.Size([1, 50, 734, 2])\n",
      "torch.Size([1, 50, 50]) torch.Size([1, 50, 50, 4]) torch.Size([1, 50, 1]) torch.Size([1, 808, 808, 4]) torch.Size([1, 808, 1]) torch.Size([1, 50, 808, 2])\n",
      "torch.Size([1, 50, 50]) torch.Size([1, 50, 50, 4]) torch.Size([1, 50, 1]) torch.Size([1, 742, 742, 4]) torch.Size([1, 742, 1]) torch.Size([1, 50, 742, 2])\n",
      "torch.Size([1, 50, 50]) torch.Size([1, 50, 50, 4]) torch.Size([1, 50, 1]) torch.Size([1, 788, 788, 4]) torch.Size([1, 788, 1]) torch.Size([1, 50, 788, 2])\n",
      "torch.Size([1, 50, 50]) torch.Size([1, 50, 50, 4]) torch.Size([1, 50, 1]) torch.Size([1, 768, 768, 4]) torch.Size([1, 768, 1]) torch.Size([1, 50, 768, 2])\n",
      "torch.Size([1, 50, 50]) torch.Size([1, 50, 50, 4]) torch.Size([1, 50, 1]) torch.Size([1, 782, 782, 4]) torch.Size([1, 782, 1]) torch.Size([1, 50, 782, 2])\n",
      "torch.Size([1, 50, 50]) torch.Size([1, 50, 50, 4]) torch.Size([1, 50, 1]) torch.Size([1, 702, 702, 4]) torch.Size([1, 702, 1]) torch.Size([1, 50, 702, 2])\n",
      "torch.Size([1, 50, 50]) torch.Size([1, 50, 50, 4]) torch.Size([1, 50, 1]) torch.Size([1, 752, 752, 4]) torch.Size([1, 752, 1]) torch.Size([1, 50, 752, 2])\n",
      "torch.Size([1, 50, 50]) torch.Size([1, 50, 50, 4]) torch.Size([1, 50, 1]) torch.Size([1, 706, 706, 4]) torch.Size([1, 706, 1]) torch.Size([1, 50, 706, 2])\n",
      "torch.Size([1, 50, 50]) torch.Size([1, 50, 50, 4]) torch.Size([1, 50, 1]) torch.Size([1, 750, 750, 4]) torch.Size([1, 750, 1]) torch.Size([1, 50, 750, 2])\n",
      "torch.Size([1, 50, 50]) torch.Size([1, 50, 50, 4]) torch.Size([1, 50, 1]) torch.Size([1, 778, 778, 4]) torch.Size([1, 778, 1]) torch.Size([1, 50, 778, 2])\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-17-e835a2af73db>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m         \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclip_grad_norm_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclip_grad_norm\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m         \u001B[0mwriter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_scalar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'train/loss'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_dataloader\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     13\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/torch/optim/adamax.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     76\u001B[0m                 \u001B[0;31m# Update biased first moment estimate.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 77\u001B[0;31m                 \u001B[0mexp_avg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmul_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbeta1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgrad\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0malpha\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mbeta1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     78\u001B[0m                 \u001B[0;31m# Update the exponentially weighted infinity norm.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     79\u001B[0m                 norm_buf = torch.cat([\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "name = f'GNN4CD_SBM({n}, {k}, {p_in:.2f}, {p_out:.2f})_bs=2'\n",
    "writer = SummaryWriter(f'./logs/{strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())} {name}')\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    model.train()\n",
    "    for it, (W, labels) in enumerate(tqdm(train_dataloader, desc=str(epoch))):\n",
    "        WW, x, WW_lg, y, P = get_lg_inputs(W.numpy(), args.J)\n",
    "        print(W.shape, WW.shape, x.shape, WW_lg.shape, y.shape, P.shape)\n",
    "        WW, x, WW_lg, y, P, labels = [x.float().to(device) for x in (WW, x, WW_lg, y, P, labels)]\n",
    "        pred = model(WW, x, WW_lg, y, P)\n",
    "\n",
    "        loss = compute_loss_multiclass(pred, labels, args.n_classes)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        writer.add_scalar('train/loss', loss.item(), epoch * len(train_dataloader) + it)\n",
    "        writer.add_scalar('train/acc', combinatorical_accuracy(pred, labels), epoch * len(train_dataloader) + it)\n",
    "        writer.add_scalar('train/ari', ari_score(pred, labels), epoch * len(train_dataloader) + it)\n",
    "        if it % 100 == 0:\n",
    "            writer.flush()\n",
    "\n",
    "#     model.eval()\n",
    "    loss_lst, acc_lst, ari_lst = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for _, (W, labels) in enumerate(test_dataloader):\n",
    "            WW, x, WW_lg, y, P = get_lg_inputs(W.numpy(), args.J)\n",
    "            WW, x, WW_lg, y, P, labels = [x.float().to(device) for x in (WW, x, WW_lg, y, P, labels)]\n",
    "            pred = model(WW, x, WW_lg, y, P)\n",
    "\n",
    "            loss = compute_loss_multiclass(pred, labels, args.n_classes)\n",
    "            loss_lst.append(loss.item())\n",
    "            acc_lst.append(combinatorical_accuracy(pred, labels))\n",
    "            ari_lst.append(ari_score(pred, labels))\n",
    "    writer.add_scalar('test/loss', np.mean(loss_lst), epoch)\n",
    "    writer.add_scalar('test/acc', np.mean(acc_lst), epoch)\n",
    "    writer.add_scalar('test/ari', np.mean(ari_lst), epoch)\n",
    "    writer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}